{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a432d7a8",
   "metadata": {},
   "source": [
    "1. Data Ingestion Pipeline:\n",
    "\n",
    "a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.\n",
    "\n",
    "b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\n",
    "\n",
    "c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Collect data from a RESTful API\n",
    "api_url = \"https://api.example.com/data\"\n",
    "response = requests.get(api_url)\n",
    "data = response.json()\n",
    "\n",
    "# Store data in a database\n",
    "# Assuming you have a database connection already established\n",
    "db.insert(data)\n",
    "\n",
    "# Collect data from a streaming platform (e.g., Apache Kafka)\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('sensor_data_topic', bootstrap_servers='localhost:9092')\n",
    "for message in consumer:\n",
    "    data = json.loads(message.value)\n",
    "    # Store data in a database or perform real-time processing\n",
    "\n",
    "# Collect data from a database (SQL)\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM sensor_data\")\n",
    "data = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a69e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer('sensor_data_topic', bootstrap_servers='localhost:9092')\n",
    "for message in consumer:\n",
    "    data = json.loads(message.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Handle CSV file\n",
    "with open('data.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        # Perform data validation and cleansing\n",
    "        # Store or process the data further\n",
    "\n",
    "# Handle JSON file\n",
    "with open('data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    # Perform data validation and cleansing\n",
    "    # Store or process the data further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1603ec",
   "metadata": {},
   "source": [
    "2. Model Training:\n",
    "\n",
    "a. Build a machine learning model to predict customer churn based on a given dataset. Train the model using appropriate algorithms and evaluate its performance.\n",
    "\n",
    "b. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, feature scaling, and dimensionality reduction.\n",
    "\n",
    "c. Train a deep learning model for image classification using transfer learning and fine-tuning techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset (assuming you have a CSV file 'churn_data.csv')\n",
    "data = pd.read_csv('churn_data.csv')\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "X = data.drop('churn', axis=1)\n",
    "y = data['churn']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model training (assuming you want to use logistic regression)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ad2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset with features (X) and target variable (y)\n",
    "# Assuming X contains both categorical and numerical features\n",
    "\n",
    "# Define preprocessing steps\n",
    "categorical_features = ['category1', 'category2']\n",
    "categorical_transformer = OneHotEncoder()\n",
    "\n",
    "numerical_features = ['numerical1', 'numerical2']\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Add dimensionality reduction step (PCA)\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dim_reduction', pca),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the pipeline (preprocessing and model training)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad858a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load pre-trained VGG16 model (assuming you have the weights stored)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model's layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the model architecture\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Prepare image data generators (assuming you have train and validation directories)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train_dir',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'val_dir',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')\n",
    "\n",
    "# Train the model with transfer learning\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator))\n",
    "\n",
    "# Fine-tuning (optional)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model after fine-tuning\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f61be",
   "metadata": {},
   "source": [
    "3. Model Validation:\n",
    "\n",
    "a. Implement cross-validation to evaluate the performance of a regression model for predicting housing prices.\n",
    "\n",
    "b. Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary classification problem.\n",
    "\n",
    "c. Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba01a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset and split into features (X) and target variable (y)\n",
    "# Assuming you have X and y\n",
    "\n",
    "# Create a regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform cross-validation (assuming you have X and y)\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate root mean squared error (RMSE) from the cross-validation scores\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "\n",
    "# Print the RMSE scores for each fold\n",
    "for fold, score in enumerate(rmse_scores):\n",
    "    print(f\"Fold {fold+1}: RMSE = {score}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of the RMSE scores\n",
    "mean_rmse = np.mean(rmse_scores)\n",
    "std_rmse = np.std(rmse_scores)\n",
    "\n",
    "print(\"Mean RMSE:\", mean_rmse)\n",
    "print(\"Standard Deviation of RMSE:\", std_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset and split into features (X) and target variable (y)\n",
    "# Assuming you have X and y for binary classification\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a classification model (assuming you want to use logistic regression)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b52ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset and split into features (X) and target variable (y)\n",
    "# Assuming you have X and y for binary classification with imbalanced classes\n",
    "\n",
    "# Create a classification model (assuming you want to use Support Vector Machine)\n",
    "model = SVC()\n",
    "\n",
    "# Create a stratified k-fold cross-validator\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Perform model validation with stratified sampling\n",
    "accuracy_scores = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print the accuracy scores for each fold\n",
    "for fold, score in enumerate(accuracy_scores):\n",
    "    print(f\"Fold {fold+1}: Accuracy = {score}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of the accuracy scores\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "std_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"Standard Deviation of Accuracy:\", std_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644f735",
   "metadata": {},
   "source": [
    "4. Deployment Strategy:\n",
    "\n",
    "   a. Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions.\n",
    "   \n",
    "   b. Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure.\n",
    "   \n",
    "   c. Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from recommendation_model import RecommendationModel  # Assuming you have a recommendation model module\n",
    "from user_interaction_stream import UserInteractionStream  # Assuming you have a user interaction stream module\n",
    "from recommendation_service import RecommendationService  # Assuming you have a recommendation service module\n",
    "\n",
    "# Load and initialize the recommendation model\n",
    "model = RecommendationModel()\n",
    "model.load_model('model_file.pkl')\n",
    "\n",
    "# Set up a user interaction stream to capture real-time data\n",
    "stream = UserInteractionStream()\n",
    "\n",
    "# Initialize the recommendation service\n",
    "service = RecommendationService(model)\n",
    "\n",
    "# Start capturing and processing real-time user interactions\n",
    "while True:\n",
    "    user_interaction = stream.get_next_interaction()\n",
    "    recommendation = service.get_recommendation(user_interaction)\n",
    "    # Perform actions with the recommendation, such as displaying it to the user or storing it in a database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "\n",
    "# Set AWS credentials and region\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'your_aws_access_key_id'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'your_aws_secret_access_key'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'your_aws_default_region'\n",
    "\n",
    "# Build the model code\n",
    "subprocess.call('python setup.py sdist', shell=True)\n",
    "\n",
    "# Upload the model code to S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file('dist/model_code.tar.gz', 'your_s3_bucket', 'model_code.tar.gz')\n",
    "\n",
    "# Create an AWS Lambda function\n",
    "lambda_client = boto3.client('lambda')\n",
    "lambda_client.create_function(\n",
    "    FunctionName='your_function_name',\n",
    "    Runtime='python3.8',\n",
    "    Role='your_lambda_role_arn',\n",
    "    Handler='handler_function_name',\n",
    "    Code={\n",
    "        'S3Bucket': 'your_s3_bucket',\n",
    "        'S3Key': 'model_code.tar.gz',\n",
    "    },\n",
    "    Environment={\n",
    "        'Variables': {\n",
    "            'ENV_VAR_NAME': 'env_var_value',\n",
    "            # Add other environment variables as required\n",
    "        }\n",
    "    },\n",
    "    Timeout=60,\n",
    "    MemorySize=512,\n",
    "    Publish=True\n",
    ")\n",
    "\n",
    "# Create an API Gateway\n",
    "api_client = boto3.client('apigateway')\n",
    "api_client.create_rest_api(\n",
    "    name='your_api_name',\n",
    "    description='your_api_description'\n",
    "    # Add other API Gateway configurations as required\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='model_logs.log', level=logging.INFO)\n",
    "\n",
    "# Monitor the deployed model\n",
    "while True:\n",
    "    try:\n",
    "        # Monitor performance metrics and log them\n",
    "        performance_metrics = monitor_performance()\n",
    "        logging.info(performance_metrics)\n",
    "\n",
    "        # Check for data drift and trigger retraining if necessary\n",
    "        if check_data_drift():\n",
    "            retrain_model()\n",
    "\n",
    "        # Sleep for a specific interval before monitoring again\n",
    "        time.sleep(60)\n",
    "    except Exception as e:\n",
    "        # Log any errors or exceptions encountered during monitoring\n",
    "        logging.error(str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
